# totoroid-poc
Automated streaming (output only system)

# driving questions
# In what resolution, temporal consistency, and captivating narrative can the latest open source stable diffusion model generate when executing on enterprise level compute vs. consumer level compute?
# How much time is required for this generated video output to be **content-positive*** - (the stream is endless: has less frames leaving memory and into a **stream-output** than the average frames generated into memory for any given period of time)
# Can this content-positive automated stream-output earn the maximum number of **ERN*** (eyes right now: the total number of users capable of consuming any **stream-output**)

1. collect most popular live stream data from youtube and finetune stable diffusion models at runtime, saving checkpoints when desired.
# why? live stream content is highly competative for ERN yet generally more static and convenient to train faster for stable diffusion

2. train by finetuning any stable diffusion model in RUNTIME => we want to capture the pixels from livestreams and while they are in memory, train the model, and then after enough time save the model checkpoint
# why? a deliberate choice to always have a changing training set since live content is always changing, we only need to get a sense of where we are in the evolutionary process of that content

3. generate unique agents for competition

4. inference output compiled into [content-positive stream-output]

5. relegation

